{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " # TP 6: Apprentissage profond(Deep learning)\n",
    " ## TP 6.4  : Optimisation \n",
    " \n",
    "- Régularisation :  Dropout\n",
    "- Normalisation \n",
    "    - Inputs\n",
    "    - les autres couches (batch Normalisation: avant ou après l'activation)\n",
    "- Optimiseurs: \n",
    "    - RMSProp, Adam, SGD, \n",
    "- Hyperparameter tuning : \n",
    "    - learning_rate\n",
    "    - \\#couches\n",
    "    - \\#neurones par couche\n",
    "    - taille du mini batch \n",
    "- ...\n",
    "# Exercice\n",
    "Trouver le meilleur modèle pour les données  de \"smoking\"\n",
    "Le programme doit tester les différentes configurations (et hyperparamètres).\n",
    "\n",
    "Visiter le site keras ou tensorflow pour vérifier la maniène d'utiliser ces différents paramètres\n",
    "\n",
    "PS : comparer aussi avec les modèes classiques : (KNN, Randomforest, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.815288500Z",
     "start_time": "2023-12-07T09:43:54.475949500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.831787100Z",
     "start_time": "2023-12-07T09:43:54.498813700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Lecture du dataset n permet de limiter le nombre de lignes à lire\n",
    "# Pour faciliter les tests\n",
    "\n",
    "def load_data(n):\n",
    "    data = pd.read_csv('../../data/train.csv')\n",
    "    return data[0:n]\n",
    "\n",
    "def select_variables(data):\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    y = data['smoking'] # récupérer la colonne survived et la mettre dans y\n",
    "    # récuperer le reste des données dans X utiliser la fonction titanic.drop ???, ??? )\n",
    "    X = data.drop('smoking', axis=1)\n",
    "    return X,y\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle \n",
    "avec prise d'un nombre de couches et nombre de neurones variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.833785300Z",
     "start_time": "2023-12-07T09:43:54.515836Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(nb_layers, nb_units): # créer une fonction qui prend en paramètre le nombre de couches et le nombre de neurones par couche\n",
    "    #utiliser Keras pour créer un modèle de type séquentiel\n",
    "    # pour créer un model de A a Z il faut :  Sequential()\n",
    "    # pour ajouter une couche : model.add(....)\n",
    "    # implémentation : \n",
    "    input_shape = (X_train.shape[1],)\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(nb_units, activation='relu', input_shape=input_shape))\n",
    "    for i in range(nb_layers-1):\n",
    "        model.add(keras.layers.Dense(nb_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.834785500Z",
     "start_time": "2023-12-07T09:43:54.527540100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compiler le modèle\n",
    "#Optimiserer: SGD, AdamW, adadelta, ...\n",
    "def compiler(model,optimizer,loss,metrics):\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrainement du modèle (Model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.834785500Z",
     "start_time": "2023-12-07T09:43:54.547895900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Entrainement du modele\n",
    "def train(model, X_train, y_train, epochs, batch_size):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation des performances du modèle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.834785500Z",
     "start_time": "2023-12-07T09:43:54.559001700Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluer(model, X_test, y_test):\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return score\n",
    "\n",
    "#print('Test loss     :', score[0])\n",
    "#print('Test accuracy :', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:54.834785500Z",
     "start_time": "2023-12-07T09:43:54.573243900Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualiser_confusion(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Get class labels\n",
    "    y_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_classes)\n",
    "    #disp= ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:55.180384900Z",
     "start_time": "2023-12-07T09:43:54.589614100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Préparation ds données\n",
    "# le -1 du load_data(-1) veut dire on prend toutes les lignes \n",
    "data=load_data(-1)\n",
    "# sélectionner les variables\n",
    "X,y = select_variables(data)\n",
    "\n",
    "\n",
    "def split_data(X, y):\n",
    "    # séparer les données en données d'entrainement et données de test\n",
    "    # utiliser la fonction train_test_split de sklearn\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(....)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def normaliser(X_train, X_test):\n",
    "    # normaliser les données d'entrainement et de test\n",
    "    # utiliser la fonction RobustScaler de sklearn\n",
    "    # X_train = RobustScaler().fit_transform(....)\n",
    "    # X_test = RobustScaler().fit_transform(....)\n",
    "    X_train = RobustScaler().fit_transform(X_train)\n",
    "    X_test = RobustScaler().fit_transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X,y)\n",
    "X_train, X_test = normaliser(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:55.195933900Z",
     "start_time": "2023-12-07T09:43:55.181827500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Afficher les paramètres du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:55.255339400Z",
     "start_time": "2023-12-07T09:43:55.200926500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#layers': 2, '#units': 10, 'optimizer': 'adam', 'loss': 'binary_crossentropy', 'metrics': ['accuracy'], 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# Afficher les paramètres du meilleur modèle\n",
    "best_param_model = {}\n",
    "best_param_model[\"#layers\"] = 2\n",
    "best_param_model[\"#units\"] = 10\n",
    "best_param_model[\"optimizer\"] = \"adam\"\n",
    "best_param_model[\"loss\"] = \"binary_crossentropy\"\n",
    "best_param_model[\"metrics\"] = [\"accuracy\"]\n",
    "best_param_model[\"epochs\"] = 100\n",
    "print(best_param_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# Manière itérative de tester les différents paramètres et récupérer les best params\n",
    "\n",
    "# 1. Tester les différents paramètres\n",
    "def test_param(param, param_values): # en entrée param représente le nom du paramètre à tester et param_values représente les valeurs à tester\n",
    "    best_score = 0 # on récupère le Math.max(best_score, score[1])\n",
    "    best_param = None # on récupère le paramètre du best_score\n",
    "    for param_value in param_values: # pour chaque valeur de param_values (chaque couche)\n",
    "        best_param_model[param] = param_value # on met à jour le paramètre du modèle\n",
    "        model = build_model(best_param_model[\"#layers\"], best_param_model[\"#units\"]) # on construit le modèle\n",
    "        model = compiler(model, best_param_model[\"optimizer\"], best_param_model[\"loss\"], best_param_model[\"metrics\"]) #compiler le modèle\n",
    "        # on entraine le modèle et on l'attribue à history (historique des scores)\n",
    "        history = train(model, X_train, y_train, best_param_model[\"epochs\"], 32)\n",
    "        # on évalue le modèle et on l'attribue à score\n",
    "        score = evaluer(model, X_test, y_test)\n",
    "        # on récupère le meilleur score et le meilleur paramètre\n",
    "        if score[1] > best_score:\n",
    "            best_score = score[1]\n",
    "            best_param = param_value\n",
    "    return best_param # une fois qu'on a fini d'itérer sur toutes les valeurs de param_values, on retourne le meilleur paramètre"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:55.273203Z",
     "start_time": "2023-12-07T09:43:55.221917800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:55.274213Z",
     "start_time": "2023-12-07T09:43:55.230514200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:55.292312Z",
     "start_time": "2023-12-07T09:43:55.247026100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param_model[\"#layers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Play with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:57.580408200Z",
     "start_time": "2023-12-07T09:43:55.265086100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss     : 0.8216451406478882\n",
      "Test accuracy : 0.4353395402431488\n"
     ]
    }
   ],
   "source": [
    "## sur les données de Text (X_test, y_test)\n",
    "model = build_model(best_param_model[\"#layers\"], best_param_model[\"#units\"])\n",
    "model = compiler(model, best_param_model[\"optimizer\"], best_param_model[\"loss\"], best_param_model[\"metrics\"])\n",
    "score = evaluer(model, X_test, y_test)\n",
    "print('Test loss     :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss     : 0.8216451406478882\n",
      "Test accuracy : 0.4353395402431488\n"
     ]
    }
   ],
   "source": [
    "print('Test loss     :', score[0])\n",
    "print('Test accuracy :', score[1])\n",
    "\n",
    "#Test loss     : 0.6390269994735718\n",
    "#Test accuracy : 0.6051615476608276\n",
    "\n",
    "# il faut faire mieux que ça !!\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:57.588502400Z",
     "start_time": "2023-12-07T09:43:57.566955200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# changer les paramètres : \n",
    "optimizer = \"adam\"\n",
    "loss = \"binary_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "epochs = 100\n",
    "batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T09:43:57.602710Z",
     "start_time": "2023-12-07T09:43:57.582659600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23748\\2446251381.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbuild_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompiler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_23748\\2818884306.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, X_train, y_train, epochs, batch_size)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Entrainement du modele\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1805\u001B[0m                         ):\n\u001B[0;32m   1806\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1807\u001B[1;33m                             \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1808\u001B[0m                             \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1809\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    831\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 832\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    833\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    834\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    866\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    867\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 868\u001B[1;33m       return tracing_compilation.call_function(\n\u001B[0m\u001B[0;32m    869\u001B[0m           \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_no_variable_creation_config\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    870\u001B[0m       )\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\u001B[0m in \u001B[0;36mcall_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    137\u001B[0m   \u001B[0mbound_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    138\u001B[0m   \u001B[0mflat_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munpack_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbound_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 139\u001B[1;33m   return function._call_flat(  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    140\u001B[0m       \u001B[0mflat_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    141\u001B[0m   )\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[0;32m   1321\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1322\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1323\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_inference_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall_preflattened\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1324\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1325\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001B[0m in \u001B[0;36mcall_preflattened\u001B[1;34m(self, args)\u001B[0m\n\u001B[0;32m    214\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mcall_preflattened\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mSequence\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m     \u001B[1;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 216\u001B[1;33m     \u001B[0mflat_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall_flat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    217\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpack_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mflat_outputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001B[0m in \u001B[0;36mcall_flat\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    249\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mrecord\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop_recording\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    250\u001B[0m           \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_bound_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 251\u001B[1;33m             outputs = self._bound_context.call_function(\n\u001B[0m\u001B[0;32m    252\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    253\u001B[0m                 \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001B[0m in \u001B[0;36mcall_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1484\u001B[0m     \u001B[0mcancellation_context\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcancellation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1485\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcancellation_context\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1486\u001B[1;33m       outputs = execute.execute(\n\u001B[0m\u001B[0;32m   1487\u001B[0m           \u001B[0mname\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"utf-8\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1488\u001B[0m           \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnum_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     54\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     55\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Entrainement du modèle\n",
    "model = build_model(2, 10)\n",
    "model = compiler(model, optimizer, loss, metrics)\n",
    "history = train(model, X_train, y_train, epochs, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T09:44:44.720854Z",
     "start_time": "2023-12-07T09:43:57.602710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluation du modèle\n",
    "score = evaluer(model, X_test, y_test)\n",
    "print('Test loss     :', score[0])\n",
    "print('Test accuracy :', score[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-07T09:44:44.711443300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tester test_param sur les différents paramètres de train, donc il faut utiliser les data de train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = load_data(-1)\n",
    "X_train, y_train = select_variables(train)\n",
    "X_train, X_test, y_train, y_test = split_data(X_train, y_train)\n",
    "X_train, X_test = normaliser(X_train, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-07T09:44:44.715299200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tester test_param sur les différents paramètres de train, donc il faut utiliser les data de train\n",
    "# 1. Tester les différents paramètres\n",
    "param = \"#layers\", \"#units\", \"optimizer\", \"loss\", \"metrics\", \"epochs\", \"batch_size\"\n",
    "param_values = [2, 10, \"adam\", \"binary_crossentropy\", [\"accuracy\"], 100, 32]\n",
    "best_param = test_param(param, param_values)\n",
    "print(best_param)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-07T09:44:44.718328Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X,y)\n",
    "X_train, X_test = normaliser(X_train, X_test)\n",
    "\n",
    "# Boucle de Test d'Hyperparamètres\n",
    "nb_layers_options = [2, 3, 4]\n",
    "nb_units_options = [10, 20, 30]\n",
    "learning_rate_options = [0.01, 0.001, 0.0001]\n",
    "batch_size_options = [16, 32, 64]\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for nb_layers in nb_layers_options:\n",
    "    for nb_units in nb_units_options:\n",
    "        for lr in learning_rate_options:\n",
    "            for batch_size in batch_size_options:\n",
    "                model = build_model(nb_layers, nb_units)\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "                model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                model.fit(X_train, y_train, epochs=100, batch_size=batch_size, verbose=0)\n",
    "                score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "                if score[1] > best_score:\n",
    "                    best_score = score[1]\n",
    "                    best_params = {\n",
    "                        'layers': nb_layers,\n",
    "                        'units': nb_units,\n",
    "                        'learning_rate': lr,\n",
    "                        'batch_size': batch_size\n",
    "                    }\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres:\", best_params)\n",
    "\n",
    "# Construire et Entraîner le Modèle avec les Meilleurs Hyperparamètres\n",
    "model = build_model(best_params['layers'], best_params['units'])\n",
    "optimizer = keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=best_params['batch_size'], verbose=0)\n",
    "\n",
    "# Effectuer une Prédiction\n",
    "# Ici, remplacez `new_data` par vos nouvelles données d'entrée\n",
    "new_data = np.array([/* vos nouvelles données ici */])\n",
    "new_data_normalized = RobustScaler().fit_transform(new_data.reshape(1, -1)) # Reshape si nécessaire\n",
    "prediction = model.predict(new_data_normalized)\n",
    "print(\"Prédiction (probabilité de fumer) :\", prediction[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-07T09:44:49.138649200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
